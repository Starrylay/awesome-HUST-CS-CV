{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f77531",
   "metadata": {},
   "source": [
    "# 算法b）三层卷积和池化两层全连接（参数略有变动）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b7318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9beda258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareFamily\\Anaconda\\envs\\nlplab\\lib\\site-packages\\ipykernel_launcher.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== epoch: 0 ===== test F1 is  [ 0.966  0.993  0.982  0.956  0.938  0.961  0.953  0.940  0.884  0.889] ===== test sum accuracy is  0.958\n",
      "========== epoch: 1 ===== test F1 is  [ 0.955  0.993  0.982  0.978  0.982  0.980  0.965  0.990  0.940  0.962] ===== test sum accuracy is  0.984\n",
      "========== epoch: 2 ===== test F1 is  [ 0.953  0.978  0.956  0.978  0.972  0.980  0.953  0.970  0.951  0.962] ===== test sum accuracy is  0.976\n",
      "========== epoch: 3 ===== test F1 is  [ 0.966  0.985  0.982  0.978  0.973  0.980  0.977  0.990  0.962  0.981] ===== test sum accuracy is  0.988\n",
      "========== epoch: 4 ===== test F1 is  [ 0.977  0.985  0.982  0.978  0.972  0.980  0.977  0.990  0.988  0.972] ===== test sum accuracy is  0.99\n",
      "========== epoch: 5 ===== test F1 is  [ 0.976  0.993  0.964  0.978  0.963  0.970  0.989  0.980  0.975  0.973] ===== test sum accuracy is  0.986\n",
      "========== epoch: 6 ===== test F1 is  [ 0.966  0.993  0.973  0.978  0.982  0.970  0.952  0.980  0.976  0.981] ===== test sum accuracy is  0.986\n",
      "========== epoch: 7 ===== test F1 is  [ 0.953  0.993  0.982  0.978  0.982  0.980  0.955  0.980  0.963  0.981] ===== test sum accuracy is  0.986\n",
      "========== epoch: 8 ===== test F1 is  [ 0.943  0.993  0.982  0.989  0.973  0.990  0.965  0.990  0.963  0.981] ===== test sum accuracy is  0.988\n",
      "========== epoch: 9 ===== test F1 is  [ 0.988  0.993  0.973  0.978  0.982  0.980  0.989  0.980  0.975  0.982] ===== test sum accuracy is  0.992\n",
      "========== epoch: 10 ===== test F1 is  [ 0.955  0.993  0.982  0.978  0.972  0.980  0.965  0.990  0.951  0.963] ===== test sum accuracy is  0.984\n",
      "========== epoch: 11 ===== test F1 is  [ 0.977  0.993  0.982  0.957  0.982  0.960  0.977  0.990  0.988  0.991] ===== test sum accuracy is  0.99\n",
      "========== epoch: 12 ===== test F1 is  [ 0.966  0.993  0.991  0.978  0.982  0.980  0.943  0.990  0.975  0.991] ===== test sum accuracy is  0.99\n",
      "========== epoch: 13 ===== test F1 is  [ 0.955  0.993  0.991  0.978  0.991  0.980  0.965  0.990  0.975  0.991] ===== test sum accuracy is  0.992\n",
      "========== epoch: 14 ===== test F1 is  [ 0.988  0.993  0.982  0.968  0.972  0.970  0.989  0.990  0.988  0.982] ===== test sum accuracy is  0.992\n",
      "========== epoch: 15 ===== test F1 is  [ 0.977  0.993  0.973  0.955  0.982  0.980  0.977  0.990  0.950  0.991] ===== test sum accuracy is  0.988\n",
      "========== epoch: 16 ===== test F1 is  [ 0.966  0.985  0.973  0.956  0.972  0.971  0.965  0.990  0.975  0.964] ===== test sum accuracy is  0.982\n",
      "========== epoch: 17 ===== test F1 is  [ 0.988  0.993  0.991  0.967  0.982  0.980  0.966  0.990  0.950  0.981] ===== test sum accuracy is  0.99\n",
      "========== epoch: 18 ===== test F1 is  [ 0.977  0.993  0.991  0.978  0.972  0.980  0.955  0.990  0.975  0.982] ===== test sum accuracy is  0.99\n",
      "========== epoch: 19 ===== test F1 is  [ 0.977  0.993  0.991  0.967  0.972  0.970  0.955  0.990  0.962  0.973] ===== test sum accuracy is  0.986\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "learning_rate = 1e-4\n",
    "keep_prob_rate = 0.5 #\n",
    "max_epoch = 20\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "DOWNLOAD_MNIST = False\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"using cuda\")\n",
    "\n",
    "if not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'):\n",
    "    # not mnist dir or mnist is empyt dir\n",
    "    DOWNLOAD_MNIST = True\n",
    "train_data = torchvision.datasets.MNIST(root='./mnist/',train=True, transform=torchvision.transforms.ToTensor(), download=DOWNLOAD_MNIST,)\n",
    "train_loader = Data.DataLoader(dataset = train_data ,batch_size= BATCH_SIZE ,shuffle= True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root = './mnist/',train = False)\n",
    "test_x = Variable(torch.unsqueeze(test_data.data,dim  = 1)).type(torch.cuda.FloatTensor)[:500]/255.\n",
    "test_y = test_data.targets[:500].numpy()\n",
    "\n",
    "test_y_list=np.zeros(10)\n",
    "for item in test_y:\n",
    "    test_y_list[item]+=1\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                # patch 7 * 7 ; 1  in channels ; 32 out channels ; ; stride is 1\n",
    "                # padding style is same(that means the convolution opration's input and output have the same size)\n",
    "                in_channels=1,\n",
    "                out_channels=64,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3,\n",
    "            ),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.MaxPool2d(2),  # pooling operation\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(  # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(  # ???\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.MaxPool2d(2),  # pooling operation\n",
    "        )\n",
    "        ##3 4 replace 2\n",
    "        self.conv3 = nn.Sequential(  # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.MaxPool2d(2),  # pooling operation\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(  # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.MaxPool2d(2),  # pooling operation\n",
    "        )\n",
    "        self.out1 =nn.Sequential(\n",
    "            nn.Linear(2304, 1024, bias=True), # full connect\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(keep_prob_rate)\n",
    "        self.out2 = nn.Linear(1024, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), 2304)  # flatten the output of coonv2 to (batch_size ,64 * 7 * 7)\n",
    "        out1 = self.out1(x) # full connect1\n",
    "        # out1 = F.relu(out1)\n",
    "\n",
    "        out1 = self.dropout(out1)\n",
    "        out2 = self.out2(out1)\n",
    "        output = F.softmax(out2)\n",
    "        return output\n",
    "\n",
    "def test(cnn):\n",
    "    predict = np.ones(10)# to avoid 0 being denominator(分母)\n",
    "    TP = np.zeros(10)\n",
    "    y_pre = cnn(test_x)\n",
    "    _, pre_index = torch.max(y_pre, 1)#max return first is the max value of the line,the second is the index of the max value in the line.\n",
    "    pre_index = pre_index.view(-1)\n",
    "    pre_index_cpu = pre_index.cpu()\n",
    "    prediction = pre_index_cpu.data.numpy()\n",
    "    for index, item in enumerate(prediction):\n",
    "        predict[item]+=1\n",
    "        if prediction[index]==test_y[index]:\n",
    "            TP[item]+=1\n",
    "    precision = TP/predict\n",
    "    recall = TP/test_y_list\n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "    correct = np.sum(prediction == test_y)\n",
    "    correct = correct/500\n",
    "    return F1, correct\n",
    "\n",
    "\n",
    "def train(cnn):\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    for epoch in range(max_epoch):\n",
    "        for step, (x_, y_) in enumerate(train_loader):\n",
    "            x, y = Variable(x_).to(device), Variable(y_).to(device)\n",
    "            output = cnn(x)\n",
    "            loss = loss_func(output, y)\n",
    "            optimizer.zero_grad()# clear the parameters such as weights and bias, for the function 'backward' would add the new to the last ones.\n",
    "            loss.backward()\n",
    "            optimizer.step()# conduct the gradient descent using the gradient generated in the 'backward'.\n",
    "\n",
    "            # if step != 0 and step % 100 == 0:\n",
    "            #     F1,correct = test(cnn)\n",
    "            #     np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "            #     print(\"=\" * 10, step, \"=\" * 5, \"test F1 is \",F1, \"=\" * 5,\"test sum accuracy is \", correct)\n",
    "        F1, correct = test(cnn)\n",
    "        np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "        print(\"=\" * 10, \"epoch:\",epoch, \"=\" * 5, \"test F1 is \", F1, \"=\" * 5, \"test sum accuracy is \", correct)\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    cnn = CNN()\n",
    "    cnn.to(device)\n",
    "    train(cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84885b29",
   "metadata": {},
   "source": [
    "# 算法c) 引入ResNet残差思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec689c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareFamily\\Anaconda\\envs\\nlplab\\lib\\site-packages\\ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "D:\\SoftwareFamily\\Anaconda\\envs\\nlplab\\lib\\site-packages\\ipykernel_launcher.py:158: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== epoch: 0 ===== test F1 is  [ 0.977  0.985  0.954  0.944  0.828  0.942  0.977  0.760  0.837  nan] ===== test sum accuracy is  0.866\n",
      "========== epoch: 1 ===== test F1 is  [ 0.977  0.993  0.964  0.955  0.944  0.971  0.944  0.914  0.963  0.933] ===== test sum accuracy is  0.966\n",
      "========== epoch: 2 ===== test F1 is  [ 0.955  0.957  0.933  0.978  0.972  0.980  0.932  0.980  0.962  0.991] ===== test sum accuracy is  0.974\n",
      "========== epoch: 3 ===== test F1 is  [ 0.955  0.993  0.982  0.967  0.991  0.970  0.965  0.980  0.962  0.982] ===== test sum accuracy is  0.986\n",
      "========== epoch: 4 ===== test F1 is  [ 0.976  0.993  0.982  0.989  0.982  0.971  0.966  0.980  0.975  0.982] ===== test sum accuracy is  0.99\n",
      "========== epoch: 5 ===== test F1 is  [ 0.966  0.993  0.973  0.957  0.956  0.959  0.953  0.990  0.950  0.953] ===== test sum accuracy is  0.976\n",
      "========== epoch: 6 ===== test F1 is  [ 0.988  0.993  0.991  0.978  0.972  0.980  0.977  0.990  0.976  0.972] ===== test sum accuracy is  0.992\n",
      "========== epoch: 7 ===== test F1 is  [ 0.966  0.993  0.991  0.968  0.973  0.980  0.953  0.990  0.975  0.981] ===== test sum accuracy is  0.988\n",
      "========== epoch: 8 ===== test F1 is  [ 0.977  0.993  0.991  0.978  0.991  0.980  0.977  0.990  0.988  0.991] ===== test sum accuracy is  0.996\n",
      "========== epoch: 9 ===== test F1 is  [ 0.977  0.993  0.982  0.967  0.973  0.980  0.977  0.990  0.962  0.991] ===== test sum accuracy is  0.99\n",
      "========== epoch: 10 ===== test F1 is  [ 0.977  0.993  0.991  0.945  0.982  0.960  0.977  0.990  0.976  0.981] ===== test sum accuracy is  0.988\n",
      "========== epoch: 11 ===== test F1 is  [ 0.988  0.993  0.991  0.968  0.972  0.970  0.966  0.990  0.976  0.981] ===== test sum accuracy is  0.99\n",
      "========== epoch: 12 ===== test F1 is  [ 0.988  0.993  0.982  0.966  0.954  0.971  0.989  0.980  0.941  0.972] ===== test sum accuracy is  0.984\n",
      "========== epoch: 13 ===== test F1 is  [ 0.988  0.993  0.991  0.968  0.991  0.970  0.989  0.990  0.976  0.981] ===== test sum accuracy is  0.994\n",
      "========== epoch: 14 ===== test F1 is  [ 0.977  0.993  0.982  0.957  0.973  0.970  0.955  0.990  0.976  0.981] ===== test sum accuracy is  0.986\n",
      "========== epoch: 15 ===== test F1 is  [ 0.988  0.993  0.991  0.967  0.982  0.970  0.977  0.990  0.988  0.991] ===== test sum accuracy is  0.994\n",
      "========== epoch: 16 ===== test F1 is  [ 0.966  0.993  0.982  0.978  0.972  0.970  0.953  0.990  0.988  0.982] ===== test sum accuracy is  0.988\n",
      "========== epoch: 17 ===== test F1 is  [ 0.977  0.993  0.982  0.968  0.982  0.980  0.977  0.990  0.975  0.991] ===== test sum accuracy is  0.992\n",
      "========== epoch: 18 ===== test F1 is  [ 0.966  0.993  0.982  0.957  0.982  0.960  0.965  0.990  0.964  0.972] ===== test sum accuracy is  0.984\n",
      "========== epoch: 19 ===== test F1 is  [ 0.977  0.993  0.982  0.978  0.982  0.970  0.966  0.980  0.964  0.962] ===== test sum accuracy is  0.986\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "learning_rate = 1e-4\n",
    "keep_prob_rate = 0.2 #\n",
    "max_epoch = 20\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "DOWNLOAD_MNIST = False\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"using cuda\")\n",
    "\n",
    "if not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'):\n",
    "    # not mnist dir or mnist is empyt dir\n",
    "    DOWNLOAD_MNIST = True\n",
    "train_data = torchvision.datasets.MNIST(root='./mnist/',train=True, transform=torchvision.transforms.ToTensor(), download=DOWNLOAD_MNIST,)\n",
    "train_loader = Data.DataLoader(dataset = train_data ,batch_size= BATCH_SIZE ,shuffle= True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root = './mnist/',train = False)\n",
    "test_x = Variable(torch.unsqueeze(test_data.data,dim  = 1)).type(torch.cuda.FloatTensor)[:500]/255.\n",
    "test_y = test_data.targets[:500].numpy()\n",
    "\n",
    "test_y_list=np.zeros(10)\n",
    "for item in test_y:\n",
    "    test_y_list[item]+=1\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=64,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3,\n",
    "            ),\n",
    "            nn.ReLU(),  # activation function\n",
    "            nn.MaxPool2d(2),  # pooling operation\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(  # ???\n",
    "          \n",
    "            nn.Conv2d(  # ???\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            )\n",
    "        )\n",
    "        ##3 4 replace 2\n",
    "        self.conv3 = nn.Sequential(  # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # pooling operation\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(  # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(\n",
    "                in_channels=256,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=256,\n",
    "                kernel_size=1,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "            ),\n",
    "            # nn.ReLU(),  # activation function\n",
    "            # nn.MaxPool2d(2),  # pooling operation\n",
    "        )\n",
    "\n",
    "        self.out1 =nn.Sequential(\n",
    "            nn.Linear(12544, 1024, bias=True), # full connect\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(keep_prob_rate)\n",
    "        self.out2 = nn.Linear(1024, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)#包含 relu 和 最大池化\n",
    "\n",
    "        x2_1 = self.conv2(x1)\n",
    "        x2_1 = F.relu(x2_1)\n",
    "        x2_1 = self.conv2(x2_1)\n",
    "        x2_1 = F.relu(x1 + x2_1)\n",
    "\n",
    "        x2_2 = self.conv2(x2_1)\n",
    "        x2_2 = F.relu(x2_2)\n",
    "        x2_2 = self.conv2(x2_2)\n",
    "        x2_2 = F.relu(x2_1 + x2_2)\n",
    "\n",
    "        x3_1 = self.conv3(x2_2)# 包含 relu 和 最大池化\n",
    "        x3_1 = self.conv4(x3_1)\n",
    "        x2_2 = self.conv5(x2_2)# 通道数转换64-256\n",
    "        x3_1 = F.relu(x3_1 + x2_2)\n",
    "\n",
    "        x3_2 = self.conv4(x3_1)\n",
    "        x3_2 = F.relu(x3_2)\n",
    "        x3_2 = self.conv4(x3_2)\n",
    "        x3_2 = F.relu(x3_1 + x3_2)\n",
    "\n",
    "        x3_2 = x3_2.view(x3_2.size(0), 12544)  # flatten the output of coonv2 to (batch_size ,64 * 7 * 7)\n",
    "        out1 = self.out1(x3_2) # full connect1\n",
    "\n",
    "        out1 = self.dropout(out1)\n",
    "        out2 = self.out2(out1)\n",
    "        output = F.softmax(out2)\n",
    "        return output\n",
    "\n",
    "\n",
    "def test(cnn):\n",
    "    predict = np.ones(10)# to avoid 0 being denominator(分母)\n",
    "    TP = np.zeros(10)\n",
    "    y_pre = cnn(test_x)\n",
    "    _, pre_index = torch.max(y_pre, 1)#max return first is the max value of the line,the second is the index of the max value in the line.\n",
    "    pre_index = pre_index.view(-1)\n",
    "    pre_index_cpu = pre_index.cpu()\n",
    "    prediction = pre_index_cpu.data.numpy()\n",
    "    for index, item in enumerate(prediction):\n",
    "        predict[item]+=1\n",
    "        if prediction[index]==test_y[index]:\n",
    "            TP[item]+=1\n",
    "    precision = TP/predict\n",
    "    recall = TP/test_y_list\n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "    correct = np.sum(prediction == test_y)\n",
    "    correct = correct/500\n",
    "    return F1, correct\n",
    "\n",
    "\n",
    "def train(cnn):\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    for epoch in range(max_epoch):\n",
    "        for step, (x_, y_) in enumerate(train_loader):\n",
    "            x, y = Variable(x_).to(device), Variable(y_).to(device)\n",
    "            output = cnn(x)\n",
    "            loss = loss_func(output, y)\n",
    "            optimizer.zero_grad()# clear the parameters such as weights and bias, for the function 'backward' would add the new to the last ones.\n",
    "            loss.backward()\n",
    "            optimizer.step()# conduct the gradient descent using the gradient generated in the 'backward'.\n",
    "\n",
    "            # if step != 0 and step % 100 == 0:\n",
    "            #     F1,correct = test(cnn)\n",
    "            #     np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "            #     print(\"=\" * 10, step, \"=\" * 5, \"test F1 is \",F1, \"=\" * 5,\"test sum accuracy is \", correct)\n",
    "        F1, correct = test(cnn)\n",
    "        np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "        print(\"=\" * 10, \"epoch:\",epoch, \"=\" * 5, \"test F1 is \", F1, \"=\" * 5, \"test sum accuracy is \", correct)\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    cnn = CNN()\n",
    "    cnn.to(device)\n",
    "    train(cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ebc85",
   "metadata": {},
   "source": [
    "# 算法d)更换激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5cfe55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareFamily\\Anaconda\\envs\\nlplab\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== epoch: 0 ===== test F1 is  [ 0.955  0.993  0.973  0.936  0.973  0.960  0.965  0.970  0.914  0.942] ===== test sum accuracy is  0.97\n",
      "========== epoch: 1 ===== test F1 is  [ 0.966  0.993  0.982  0.967  0.964  0.980  0.977  0.980  0.938  0.963] ===== test sum accuracy is  0.982\n",
      "========== epoch: 2 ===== test F1 is  [ 0.977  0.985  0.973  0.967  0.954  0.980  0.989  0.990  0.923  0.972] ===== test sum accuracy is  0.982\n",
      "========== epoch: 3 ===== test F1 is  [ 0.977  0.993  0.991  0.978  0.973  0.980  0.977  0.990  0.950  0.981] ===== test sum accuracy is  0.99\n",
      "========== epoch: 4 ===== test F1 is  [ 0.966  0.985  0.982  0.957  0.953  0.970  0.965  0.990  0.976  0.955] ===== test sum accuracy is  0.98\n",
      "========== epoch: 5 ===== test F1 is  [ 0.977  0.993  0.991  0.967  0.982  0.980  0.977  0.990  0.951  0.981] ===== test sum accuracy is  0.99\n",
      "========== epoch: 6 ===== test F1 is  [ 0.988  0.993  0.991  0.967  0.982  0.980  0.977  0.990  0.976  0.991] ===== test sum accuracy is  0.994\n",
      "========== epoch: 7 ===== test F1 is  [ 0.988  0.993  0.991  0.967  0.963  0.980  0.977  0.990  0.976  0.973] ===== test sum accuracy is  0.99\n",
      "========== epoch: 8 ===== test F1 is  [ 0.988  0.985  0.982  0.968  0.972  0.970  0.977  0.990  0.988  0.982] ===== test sum accuracy is  0.99\n",
      "========== epoch: 9 ===== test F1 is  [ 0.988  0.993  0.982  0.967  0.982  0.970  0.977  0.980  0.988  0.991] ===== test sum accuracy is  0.992\n",
      "========== epoch: 10 ===== test F1 is  [ 0.988  0.993  0.982  0.978  0.982  0.980  0.989  0.990  0.976  0.981] ===== test sum accuracy is  0.994\n",
      "========== epoch: 11 ===== test F1 is  [ 0.988  0.993  0.973  0.967  0.972  0.970  0.989  0.980  0.976  0.972] ===== test sum accuracy is  0.988\n",
      "========== epoch: 12 ===== test F1 is  [ 0.988  0.993  0.991  0.989  0.972  0.990  0.977  0.990  0.988  0.982] ===== test sum accuracy is  0.996\n",
      "========== epoch: 13 ===== test F1 is  [ 0.988  0.993  0.982  0.978  0.982  0.990  0.977  0.990  0.963  0.991] ===== test sum accuracy is  0.994\n",
      "========== epoch: 14 ===== test F1 is  [ 0.988  0.993  0.982  0.957  0.972  0.970  0.966  0.990  0.976  0.982] ===== test sum accuracy is  0.988\n",
      "========== epoch: 15 ===== test F1 is  [ 0.988  0.993  0.991  0.957  0.963  0.970  0.977  0.990  0.976  0.973] ===== test sum accuracy is  0.988\n",
      "========== epoch: 16 ===== test F1 is  [ 0.988  0.993  0.982  0.967  0.982  0.980  0.989  0.990  0.976  0.991] ===== test sum accuracy is  0.994\n",
      "========== epoch: 17 ===== test F1 is  [ 0.988  0.993  0.991  0.967  0.982  0.970  0.977  0.990  0.976  0.981] ===== test sum accuracy is  0.992\n",
      "========== epoch: 18 ===== test F1 is  [ 0.988  0.993  0.991  0.978  0.972  0.980  0.977  0.990  0.988  0.982] ===== test sum accuracy is  0.994\n",
      "========== epoch: 19 ===== test F1 is  [ 0.988  0.985  0.973  0.957  0.982  0.970  0.989  0.990  0.976  0.991] ===== test sum accuracy is  0.99\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "learning_rate = 1e-4\n",
    "keep_prob_rate = 0.2 #\n",
    "max_epoch = 20\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "DOWNLOAD_MNIST = False\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"using cuda\")\n",
    "\n",
    "if not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'):\n",
    "    # not mnist dir or mnist is empyt dir\n",
    "    DOWNLOAD_MNIST = True\n",
    "train_data = torchvision.datasets.MNIST(root='./mnist/',train=True, transform=torchvision.transforms.ToTensor(), download=DOWNLOAD_MNIST,)\n",
    "train_loader = Data.DataLoader(dataset = train_data ,batch_size= BATCH_SIZE ,shuffle= True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root = './mnist/',train = False)\n",
    "test_x = Variable(torch.unsqueeze(test_data.data,dim  = 1)).type(torch.cuda.FloatTensor)[:500]/255.\n",
    "test_y = test_data.targets[:500].numpy()\n",
    "\n",
    "test_y_list=np.zeros(10)\n",
    "for item in test_y:\n",
    "    test_y_list[item]+=1\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                # patch 7 * 7 ; 1  in channels ; 32 out channels  ; stride is 1\n",
    "                # padding style is same(that means the convolution opration's input and output have the same size)\n",
    "                in_channels=1,\n",
    "                out_channels=64,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3,\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.ReLU(),  # activation function\n",
    "            nn.MaxPool2d(2),  # pooling operation\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(  # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(  # ???\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            )\n",
    "        )\n",
    "        ##3 4 replace 2\n",
    "        self.conv3 = nn.Sequential(  # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # pooling operation\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(  # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(\n",
    "                in_channels=256,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=256,\n",
    "                kernel_size=1,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.out1 =nn.Sequential(\n",
    "            nn.Linear(12544, 1024, bias=True), # full connect\n",
    "            # nn.ReLU()\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(keep_prob_rate)\n",
    "        self.out2 = nn.Linear(1024, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)#包含 relu 和 最大池化\n",
    "\n",
    "        x2_1 = self.conv2(x1)\n",
    "        x2_1 = F.relu(x2_1)\n",
    "\n",
    "        x2_1 = self.conv2(x2_1)\n",
    "        x2_1 = F.relu(x1 + x2_1)\n",
    "\n",
    "        x2_2 = self.conv2(x2_1)\n",
    "        x2_2 = F.relu(x2_2)\n",
    "        x2_2 = self.conv2(x2_2)\n",
    "        x2_2 = F.relu(x2_1 + x2_2)\n",
    "\n",
    "        x3_1 = self.conv3(x2_2)# 包含 relu 和 最大池化\n",
    "        x3_1 = self.conv4(x3_1)\n",
    "        x2_2 = self.conv5(x2_2)# 通道数转换64-256\n",
    "        x3_1 = F.relu(x3_1 + x2_2)\n",
    "\n",
    "        x3_2 = self.conv4(x3_1)\n",
    "        x3_2 = F.relu(x3_2)\n",
    "        x3_2 = self.conv4(x3_2)\n",
    "        x3_2 = F.relu(x3_1 + x3_2)\n",
    "\n",
    "        x3_2 = x3_2.view(x3_2.size(0), 12544)  # flatten the output of coonv2 to (batch_size ,64 * 7 * 7)\n",
    "        out1 = self.out1(x3_2) # full connect1\n",
    "\n",
    "        out1 = self.dropout(out1)\n",
    "        out2 = self.out2(out1)\n",
    "        output = F.softmax(out2)\n",
    "        return output\n",
    "\n",
    "\n",
    "def test(cnn):\n",
    "    predict = np.ones(10)# to avoid 0 being denominator(分母)\n",
    "    TP = np.zeros(10)\n",
    "    y_pre = cnn(test_x)\n",
    "    _, pre_index = torch.max(y_pre, 1)#max return first is the max value of the line,the second is the index of the max value in the line.\n",
    "    pre_index = pre_index.view(-1)\n",
    "    pre_index_cpu = pre_index.cpu()\n",
    "    prediction = pre_index_cpu.data.numpy()\n",
    "    for index, item in enumerate(prediction):\n",
    "        predict[item]+=1\n",
    "        if prediction[index]==test_y[index]:\n",
    "            TP[item]+=1\n",
    "    precision = TP/predict\n",
    "    recall = TP/test_y_list\n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "    correct = np.sum(prediction == test_y)\n",
    "    correct = correct/500\n",
    "    return F1, correct\n",
    "\n",
    "\n",
    "def train(cnn):\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    for epoch in range(max_epoch):\n",
    "        for step, (x_, y_) in enumerate(train_loader):\n",
    "            x, y = Variable(x_).to(device), Variable(y_).to(device)\n",
    "            output = cnn(x)\n",
    "            loss = loss_func(output, y)\n",
    "            optimizer.zero_grad()# clear the parameters such as weights and bias, for the function 'backward' would add the new to the last ones.\n",
    "            loss.backward()\n",
    "            optimizer.step()# conduct the gradient descent using the gradient generated in the 'backward'.\n",
    "\n",
    "            # if step != 0 and step % 100 == 0:\n",
    "            #     F1,correct = test(cnn)\n",
    "            #     np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "            #     print(\"=\" * 10, step, \"=\" * 5, \"test F1 is \",F1, \"=\" * 5,\"test sum accuracy is \", correct)\n",
    "        F1, correct = test(cnn)\n",
    "        np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "        print(\"=\" * 10, \"epoch:\",epoch, \"=\" * 5, \"test F1 is \", F1, \"=\" * 5, \"test sum accuracy is \", correct)\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    cnn = CNN()\n",
    "    cnn.to(device)\n",
    "    train(cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18ee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlplab",
   "language": "python",
   "name": "nlplab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
